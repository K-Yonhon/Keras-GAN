{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/keras-team/keras-contrib/blob/master/examples/improved_wgan.py\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wasserstein_loss(y_true, y_pred):\n",
    "    \"\"\"Calculates the Wasserstein loss for a sample batch.\n",
    "    The Wasserstein loss function is very simple to calculate. In a standard GAN, the\n",
    "    discriminator has a sigmoid output, representing the probability that samples are\n",
    "    real or generated. In Wasserstein GANs, however, the output is linear with no\n",
    "    activation function! Instead of being constrained to [0, 1], the discriminator wants\n",
    "    to make the distance between its output for real and generated samples as\n",
    "    large as possible.\n",
    "    The most natural way to achieve this is to label generated samples -1 and real\n",
    "    samples 1, instead of the 0 and 1 used in normal GANs, so that multiplying the\n",
    "    outputs by the labels will give you the loss immediately.\n",
    "    Note that the nature of this loss means that it can be (and frequently will be)\n",
    "    less than 0.\"\"\"\n",
    "    return K.mean(y_true * y_pred)\n",
    "\n",
    "\n",
    "def gradient_penalty_loss(y_true, y_pred, averaged_samples,\n",
    "                          gradient_penalty_weight):\n",
    "    \"\"\"Calculates the gradient penalty loss for a batch of \"averaged\" samples.\n",
    "    In Improved WGANs, the 1-Lipschitz constraint is enforced by adding a term to the\n",
    "    loss function that penalizes the network if the gradient norm moves away from 1.\n",
    "    However, it is impossible to evaluate this function at all points in the input\n",
    "    space. The compromise used in the paper is to choose random points on the lines\n",
    "    between real and generated samples, and check the gradients at these points. Note\n",
    "    that it is the gradient w.r.t. the input averaged samples, not the weights of the\n",
    "    discriminator, that we're penalizing!\n",
    "    In order to evaluate the gradients, we must first run samples through the generator\n",
    "    and evaluate the loss. Then we get the gradients of the discriminator w.r.t. the\n",
    "    input averaged samples. The l2 norm and penalty can then be calculated for this\n",
    "    gradient.\n",
    "    Note that this loss function requires the original averaged samples as input, but\n",
    "    Keras only supports passing y_true and y_pred to loss functions. To get around this,\n",
    "    we make a partial() of the function with the averaged_samples argument, and use that\n",
    "    for model training.\"\"\"\n",
    "    # first get the gradients:\n",
    "    #   assuming: - that y_pred has dimensions (batch_size, 1)\n",
    "    #             - averaged_samples has dimensions (batch_size, nbr_features)\n",
    "    # gradients afterwards has dimension (batch_size, nbr_features), basically\n",
    "    # a list of nbr_features-dimensional gradient vectors\n",
    "    gradients = K.gradients(y_pred, averaged_samples)[0]\n",
    "    # compute the euclidean norm by squaring ...\n",
    "    gradients_sqr = K.square(gradients)\n",
    "    #   ... summing over the rows ...\n",
    "    gradients_sqr_sum = K.sum(gradients_sqr,\n",
    "                              axis=np.arange(1, len(gradients_sqr.shape)))\n",
    "    #   ... and sqrt\n",
    "    gradient_l2_norm = K.sqrt(gradients_sqr_sum)\n",
    "    # compute lambda * (1 - ||grad||)^2 still for each single sample\n",
    "    gradient_penalty = gradient_penalty_weight * K.square(1 - gradient_l2_norm)\n",
    "    # return the mean as loss over all the batch samples\n",
    "    return K.mean(gradient_penalty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
