{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Input, Reshape, Flatten, Activation\n",
    "from keras.layers.convolutional import Conv2D, Conv2DTranspose\n",
    "from keras.layers.advanced_activations import ReLU, LeakyReLU\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras.utils import multi_gpu_model\n",
    "\n",
    "import keras\n",
    "import keras.backend as K\n",
    "import keras.backend.tensorflow_backend as KTF\n",
    "import tensorflow as tf\n",
    "\n",
    "import shutil, os, sys, io, random, math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import partial\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "os.chdir('/home/k_yonhon/py/Keras-GAN/pggan/')\n",
    "sys.path.append(os.pardir)\n",
    "\n",
    "from tensor_board_logger import TensorBoardLogger\n",
    "from layer_visualizer import LayerVisualizer\n",
    "\n",
    "config = tf.ConfigProto(gpu_options=tf.GPUOptions(allow_growth=True))\n",
    "session = tf.Session(config=config)\n",
    "KTF.set_session(session)\n",
    "\n",
    "gpu_count = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wasserstein_loss(y_true, y_pred):\n",
    "    return K.mean(y_true * y_pred)\n",
    "\n",
    "def gradient_penalty_loss(y_true, y_pred, \n",
    "                          averaged_samples,\n",
    "                          gradient_penalty_weight):\n",
    "    gradients = K.gradients(y_pred, averaged_samples)[0]\n",
    "    gradients_sqr = K.square(gradients)\n",
    "    gradients_sqr_sum = K.sum(gradients_sqr, \n",
    "                              axis=np.arange(1, len(gradients_sqr.shape)))\n",
    "    gradient_l2_norm = K.sqrt(gradients_sqr_sum)\n",
    "    gradient_penalty = gradient_penalty_weight * K.square(gradient_l2_norm - 1)\n",
    "    return K.mean(gradient_penalty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WGANGP():\n",
    "    def __init__(self):\n",
    "        # ---------------------\n",
    "        #  for log on TensorBoard\n",
    "        # ---------------------\n",
    "        target_dir = \"./my_log_dir\"\n",
    "        shutil.rmtree(target_dir, ignore_errors=True)\n",
    "        os.mkdir(target_dir)\n",
    "        self.logger = TensorBoardLogger(log_dir=target_dir)\n",
    "\n",
    "        # ---------------------\n",
    "        #  Parameter\n",
    "        # ---------------------      \n",
    "        self.img_rows = 64\n",
    "        self.img_cols = 64\n",
    "        self.channels = 3\n",
    "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
    "\n",
    "        self.input_rows = 4\n",
    "        self.input_cols = 4\n",
    "        self.latent_dim = 128  # Noiseの次元\n",
    "        \n",
    "        self.n_critic = 5\n",
    "        self.λ = 10        \n",
    "        optimizer = Adam(lr=0.0001, beta_1=0., beta_2=0.9, epsilon=None, decay=0.0, amsgrad=False)\n",
    "\n",
    "        # ---------------------\n",
    "        #  Build model\n",
    "        # ---------------------\n",
    "        self.critic = self.build_critic()\n",
    "        self.generator = self.build_generator()\n",
    "        \n",
    "        #  Load pretrained weights        \n",
    "        pre_gen = load_model('./saved_model/wgangp32_gen_model.h5')\n",
    "        for i, layer in enumerate(self.generator.layers[1].layers):\n",
    "            if i in [i for i in range(1, int(math.log(self.img_rows / self.input_rows, 2)) * 2, 2)]:\n",
    "                layer.set_weights(pre_gen.layers[1].layers[i].get_weights())\n",
    "                layer.trainable = False\n",
    "                \n",
    "        pre_critic = load_model('./saved_model/wgangp32_critic_model.h5')\n",
    "        for i, layer in enumerate(self.critic.layers[1].layers):\n",
    "            j = i - len(self.critic.layers[1].layers)\n",
    "            if j in [-i for i in range(int(math.log(self.img_rows / self.input_rows, 2)) * 2, 0, -2)]:\n",
    "                layer.set_weights(pre_critic.layers[1].layers[j].get_weights())\n",
    "                layer.trainable = False\n",
    "\n",
    "        #-------------------------------\n",
    "        # Construct Computational Graph\n",
    "        #       for the Critic\n",
    "        #-------------------------------    \n",
    "        generated_samples = Input(shape=self.img_shape) \n",
    "        critic_output_from_generated_samples = self.critic(generated_samples)\n",
    "        \n",
    "        real_samples = Input(shape=self.img_shape)        \n",
    "        critic_output_from_real_samples = self.critic(real_samples)\n",
    "\n",
    "        averaged_samples = Input(shape=self.img_shape)\n",
    "        critic_output_from_averaged_samples = self.critic(averaged_samples)\n",
    "\n",
    "        partial_gp_loss = partial(gradient_penalty_loss,\n",
    "                                  averaged_samples=averaged_samples,\n",
    "                                  gradient_penalty_weight=self.λ)\n",
    "        # Functions need names or Keras will throw an error\n",
    "        partial_gp_loss.__name__ = 'gradient_penalty'\n",
    "\n",
    "        self.critic_model = Model(inputs=[generated_samples, \n",
    "                                          real_samples,\n",
    "                                          averaged_samples],\n",
    "                                  outputs=[critic_output_from_generated_samples, \n",
    "                                           critic_output_from_real_samples,\n",
    "                                           critic_output_from_averaged_samples])\n",
    "        if gpu_count > 1:\n",
    "            self.critic_model = multi_gpu_model(self.critic_model, gpus=gpu_count)\n",
    "        self.critic_model.compile(optimizer=optimizer, \n",
    "                                  loss=[wasserstein_loss, \n",
    "                                        wasserstein_loss, \n",
    "                                        partial_gp_loss])\n",
    "        \n",
    "        print('Critic Summary:')\n",
    "        self.critic.summary()       \n",
    "        \n",
    "        #-------------------------------\n",
    "        # Construct Computational Graph\n",
    "        #         for Generator\n",
    "        #-------------------------------\n",
    "        # For the generator we freeze the critic's layers\n",
    "        self.critic.trainable = False\n",
    "                    \n",
    "        generator_input = Input(shape=(self.latent_dim,))\n",
    "        generator_layers = self.generator(generator_input)\n",
    "        critic_layers_for_generator = self.critic(generator_layers)\n",
    "        \n",
    "        self.generator_model = Model(inputs=[generator_input], \n",
    "                                     outputs=[critic_layers_for_generator])\n",
    "        if gpu_count > 1:\n",
    "            self.generator_model = multi_gpu_model(self.generator_model, gpus=gpu_count)\n",
    "        self.generator_model.compile(optimizer=optimizer,\n",
    "                                     loss=wasserstein_loss)        \n",
    "\n",
    "        print('Genarator Summary:')\n",
    "        self.generator.summary()   \n",
    "\n",
    "    def build_generator(self):\n",
    "        model = Sequential()\n",
    "        model.add(Reshape((self.input_rows, self.input_cols, int(self.latent_dim / (self.input_rows * self.input_cols))), \n",
    "                          input_shape=(self.latent_dim,)\n",
    "                         ))\n",
    "\n",
    "        model.add(Conv2DTranspose(512, (3, 3), strides=1, padding='same',\n",
    "                                 kernel_initializer=keras.initializers.Orthogonal(gain=1.4, seed=None),\n",
    "                                 ))\n",
    "        model.add(LeakyReLU(alpha=0.2))      \n",
    "\n",
    "        for _ in range(int(math.log(self.img_rows / self.input_rows, 2))):\n",
    "            model.add(Conv2DTranspose(512, (3, 3), strides=2, padding='same', \n",
    "                                     kernel_initializer=keras.initializers.Orthogonal(gain=1.4, seed=None),\n",
    "                                     ))\n",
    "            model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "        model.add(Conv2DTranspose(256, (3, 3), strides=1, padding='same', \n",
    "                                  kernel_initializer=keras.initializers.Orthogonal(gain=1.4, seed=None),\n",
    "                                  ))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "        model.add(Conv2DTranspose(128, (3, 3), strides=1, padding='same', \n",
    "                                 kernel_initializer=keras.initializers.Orthogonal(gain=1.4, seed=None),\n",
    "                                 ))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "        model.add(Conv2DTranspose(64, (3, 3), strides=1, padding='same', \n",
    "                                 kernel_initializer=keras.initializers.Orthogonal(gain=1.4, seed=None),\n",
    "                                 ))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "        model.add(Conv2DTranspose(32, (3, 3), strides=1, padding='same', \n",
    "                                 kernel_initializer=keras.initializers.Orthogonal(gain=1.4, seed=None),\n",
    "                                 ))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "        model.add(Conv2DTranspose(16, (3, 3), strides=1, padding='same', \n",
    "                                 kernel_initializer=keras.initializers.Orthogonal(gain=1.4, seed=None),\n",
    "                                 ))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "        model.add(Conv2DTranspose(3, (3, 3), strides=1, padding='same', \n",
    "                                 kernel_initializer=keras.initializers.Orthogonal(gain=1.4, seed=None),\n",
    "                                 ))                \n",
    "        model.add(Activation(\"tanh\"))\n",
    "\n",
    "        noise = Input(shape=(self.latent_dim,))\n",
    "        img = model(noise)\n",
    "        return Model(noise, img)\n",
    "    \n",
    "    def build_critic(self):\n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(16, (1, 1), strides=1, input_shape=self.img_shape, padding=\"valid\",\n",
    "                         kernel_initializer=keras.initializers.Orthogonal(gain=1.4, seed=None),\n",
    "                        ))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "        model.add(Conv2D(32, (3, 3), strides=1, padding=\"same\",\n",
    "                         kernel_initializer=keras.initializers.Orthogonal(gain=1.4, seed=None),\n",
    "                        ))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "        model.add(Conv2D(64, (3, 3), strides=1, padding=\"same\",\n",
    "                         kernel_initializer=keras.initializers.Orthogonal(gain=1.4, seed=None),\n",
    "                        ))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "        model.add(Conv2D(128, (3, 3), strides=1, padding=\"same\",\n",
    "                         kernel_initializer=keras.initializers.Orthogonal(gain=1.4, seed=None),\n",
    "                        ))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "        model.add(Conv2D(256, (3, 3), strides=1, padding=\"same\",\n",
    "                         kernel_initializer=keras.initializers.Orthogonal(gain=1.4, seed=None),\n",
    "                        ))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "        model.add(Conv2D(512, (3, 3), strides=1, padding=\"same\",\n",
    "                         kernel_initializer=keras.initializers.Orthogonal(gain=1.4, seed=None),\n",
    "                        ))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "        for _ in range(int(math.log(self.img_rows / self.input_rows, 2))):\n",
    "            model.add(Conv2D(512, (3, 3), strides=2, padding=\"same\",\n",
    "                             kernel_initializer=keras.initializers.Orthogonal(gain=1.4, seed=None),\n",
    "                            ))\n",
    "            model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "        model.add(Conv2D(1, (4, 4), strides=1, padding=\"valid\",\n",
    "                         kernel_initializer=keras.initializers.Orthogonal(gain=1.4, seed=None),\n",
    "                        ))\n",
    "        model.add(Flatten())\n",
    "\n",
    "        img = Input(shape=self.img_shape)\n",
    "        validity = model(img)\n",
    "        return Model(img, validity)\n",
    "    \n",
    "    def train(self, epochs, batch_size, sample_interval=50, resume=0):\n",
    "        # Load suspended training weights\n",
    "        if resume != 0:\n",
    "            self.critic = load_model('./saved_model/wgangp64_critic_model_'+str(resume)+'epoch.h5')\n",
    "            self.generator = load_model('./saved_model/wgangp64_gen_model_'+str(resume)+'epoch.h5')\n",
    "        \n",
    "        # Load the dataset\n",
    "        X_train = np.load('../datasets/lfw64.npz')['arr_0']\n",
    "        X_train = X_train / 127.5 - 1.0   # Rescale -1 to 1\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        valid = -np.ones((batch_size, 1), dtype=np.float32)\n",
    "        fake = np.ones((batch_size, 1), dtype=np.float32)\n",
    "        dummy = np.zeros((batch_size, 1), dtype=np.float32)\n",
    "\n",
    "        # ---------------------\n",
    "        #  Training\n",
    "        # ---------------------\n",
    "        for epoch in tqdm(range(resume, resume + epochs + 1)):\n",
    "            for _ in range(self.n_critic):\n",
    "                noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
    "                gen_imgs = self.generator.predict(noise, batch_size=batch_size)\n",
    "                \n",
    "                idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "                real_imgs = X_train[idx]\n",
    "                               \n",
    "                ε = np.random.uniform(size=(batch_size, 1,1,1))\n",
    "                ave_imgs = ε * real_imgs + (1-ε) * gen_imgs\n",
    "                \n",
    "                # Train Critic\n",
    "                d_loss = self.critic_model.train_on_batch([gen_imgs, real_imgs, ave_imgs], \n",
    "                                                          [fake, valid, dummy])\n",
    "\n",
    "            # Train Generator\n",
    "            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
    "            g_loss = self.generator_model.train_on_batch(noise, valid)\n",
    "            \n",
    "            # Backup Model\n",
    "            if epoch != 0 and epoch % 1000 == 0:\n",
    "                self.critic.save('./saved_model/wgangp64_critic_model_'+str(epoch+resume)+'epoch.h5')\n",
    "                self.generator.save('./saved_model/wgangp64_gen_model_'+str(epoch+resume)+'epoch.h5')\n",
    "            \n",
    "            # Log Loss & Histgram\n",
    "            logs = {\n",
    "                \"loss/Critic\": d_loss[0],\n",
    "                \"loss/Generator\": g_loss,\n",
    "                \"loss_Critic/D_gen\": d_loss[1],\n",
    "                \"loss_Critic/D_real\": -d_loss[2],\n",
    "                \"loss_Critic/gradient_penalty\": d_loss[3],\n",
    "                \"loss_Critic/total_loss\": d_loss[1] + d_loss[2] + d_loss[3],                \n",
    "            }\n",
    "\n",
    "            histograms = {}\n",
    "            for layer in self.critic.layers[1].layers:\n",
    "                for i in range(len(layer.get_weights())):\n",
    "                    if \"conv\" in layer.name or \"dense\" in layer.name:\n",
    "                        name = layer.name + \"/\" + str(i)\n",
    "                        value = layer.get_weights()[i]\n",
    "                        histograms[name] = value\n",
    "            \n",
    "            self.logger.log(logs=logs, histograms=histograms, epoch=epoch+resume)\n",
    "            \n",
    "            # Log generated image samples\n",
    "            if epoch+resume == 1000 or epoch+resume == 2000 or (epoch+resume) % sample_interval == 0:\n",
    "                fig, name = self.sample_images(epoch+resume, dataset=X_train)\n",
    "                images = {name: fig}\n",
    "                self.logger.log(images=images, epoch=epoch+resume)\n",
    "                print(\"%d [C loss: %f] [G loss: %f]\" % (epoch, d_loss[0], g_loss))\n",
    "\n",
    "    def sample_images(self, epoch, dataset):\n",
    "        r, c = 3, 3\n",
    "        if epoch == 0:\n",
    "            idx = np.random.randint(0, dataset.shape[0], r * c)\n",
    "            imgs = dataset[idx]\n",
    "        else:\n",
    "            noise = np.random.normal(0, 1, (r * c, self.latent_dim))\n",
    "            imgs = self.generator.predict(noise, batch_size=r * c)\n",
    "\n",
    "        # Rescale images 0 - 255\n",
    "        imgs = ((0.5 * imgs + 0.5) * 255).astype(np.uint8)\n",
    "        fig, axs = plt.subplots(r, c)\n",
    "        cnt = 0\n",
    "        for i in range(r):\n",
    "            for j in range(c):\n",
    "                if self.channels == 1:\n",
    "                    axs[i, j].imshow(imgs[cnt, :, :, 0], cmap=\"gray\")\n",
    "                else:\n",
    "                    axs[i, j].imshow(imgs[cnt, :, :, :self.channels], cmap=\"gray\")\n",
    "                axs[i, j].axis(\"off\")\n",
    "                cnt += 1\n",
    "        if epoch == 0:\n",
    "            name = \"original.png\"\n",
    "        else:\n",
    "            name = str(epoch) + \".png\"\n",
    "        return fig, name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Critic Summary:\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_18 (InputLayer)        (None, 64, 64, 3)         0         \n",
      "_________________________________________________________________\n",
      "sequential_7 (Sequential)    (None, 1)                 11019809  \n",
      "=================================================================\n",
      "Total params: 11,019,809\n",
      "Trainable params: 3,932,192\n",
      "Non-trainable params: 7,087,617\n",
      "_________________________________________________________________\n",
      "Genarator Summary:\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_19 (InputLayer)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "sequential_8 (Sequential)    (None, 64, 64, 3)         11048867  \n",
      "=================================================================\n",
      "Total params: 11,048,867\n",
      "Trainable params: 3,932,067\n",
      "Non-trainable params: 7,116,800\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "wgan = WGANGP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wgan.train(epochs=10000, batch_size=64, sample_interval=1000, resume=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wgan.generator.save('./saved_model/wgangp64_gen_model.h5')\n",
    "wgan.critic.save('./saved_model/wgangp64_critic_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
