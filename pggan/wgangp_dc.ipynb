{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "batch_size = 32, 2GPUs| 12/5001 [04:32<27:04:03, 19.53s/it]\n",
    "batch_size = 32, 4GPUs| 17/2001 [04:27<6:56:56, 12.61s/it]\n",
    "batch_size = 64, 4GPUs| 15/1001 [06:53<6:20:48, 23.17s/it]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Input, Reshape, Flatten, Activation\n",
    "from keras.layers.convolutional import Conv2D, Conv2DTranspose\n",
    "from keras.layers.advanced_activations import ReLU, LeakyReLU\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras.utils import multi_gpu_model\n",
    "\n",
    "import keras\n",
    "import keras.backend as K\n",
    "import keras.backend.tensorflow_backend as KTF\n",
    "import tensorflow as tf\n",
    "\n",
    "import shutil, os, sys, io, random, math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import partial\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "os.chdir('/home/k_yonhon/py/Keras-GAN/pggan/')\n",
    "sys.path.append(os.pardir)\n",
    "\n",
    "from tensor_board_logger import TensorBoardLogger\n",
    "from wasserstein_loss import WassersteinLoss, GradientPenaltyLoss\n",
    "\n",
    "config = tf.ConfigProto(gpu_options=tf.GPUOptions(allow_growth=True))\n",
    "session = tf.Session(config=config)\n",
    "KTF.set_session(session)\n",
    "\n",
    "# ---------------------\n",
    "#  Parameter\n",
    "# ---------------------\n",
    "gpu_count = 2\n",
    "dataset = np.load('../datasets/lfw64.npz')['arr_0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WGANGP():\n",
    "    def __init__(self, dataset, gpu_count=1):\n",
    "        # ---------------------\n",
    "        #  Parameter\n",
    "        # ---------------------\n",
    "        self.dataset = dataset\n",
    "        self.gpu_count = gpu_count\n",
    "                \n",
    "        self.img_rows = dataset.shape[1]\n",
    "        self.img_cols = dataset.shape[2]\n",
    "        self.channels = dataset.shape[3]\n",
    "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
    "\n",
    "        self.input_rows = 4\n",
    "        self.input_cols = 4\n",
    "        self.latent_dim = 128  # Noise dim\n",
    "        \n",
    "        self.n_critic = 5\n",
    "        self.λ = 10        \n",
    "        optimizer = Adam(lr=0.0001, beta_1=0., beta_2=0.9, epsilon=None, decay=0.0, amsgrad=False)\n",
    "\n",
    "        # ---------------------\n",
    "        #  for TensorBoard\n",
    "        # ---------------------\n",
    "        target_dir = \"./my_log_dir\"\n",
    "        shutil.rmtree(target_dir, ignore_errors=True)\n",
    "        os.mkdir(target_dir)\n",
    "        self.logger = TensorBoardLogger(log_dir=target_dir)\n",
    "\n",
    "        # ---------------------\n",
    "        #  Load models\n",
    "        # ---------------------\n",
    "        self.critic = self.build_critic()\n",
    "        self.generator = self.build_generator()\n",
    "        \n",
    "        #  Load pretrained weights\n",
    "        '''\n",
    "        pre_gen = load_model('./saved_model/wgangp64_gen_model_3k.h5')\n",
    "        for i, layer in enumerate(self.generator.layers[1].layers):\n",
    "            if i in [i for i in range(1, int(math.log(self.img_rows / self.input_rows, 2)) * 2, 2)]:\n",
    "                layer.set_weights(pre_gen.layers[1].layers[i].get_weights())\n",
    "                layer.trainable = False\n",
    "                \n",
    "        pre_critic = load_model('./saved_model/wgangp64_critic_model_3k.h5')\n",
    "        for i, layer in enumerate(self.critic.layers[1].layers):\n",
    "            j = i - len(self.critic.layers[1].layers)\n",
    "            if j in [-i for i in range(int(math.log(self.img_rows / self.input_rows, 2)) * 2, 0, -2)]:\n",
    "                layer.set_weights(pre_critic.layers[1].layers[j].get_weights())\n",
    "                layer.trainable = False\n",
    "        '''\n",
    "        #-------------------------------\n",
    "        # Compile Critic\n",
    "        #-------------------------------    \n",
    "        generated_samples = Input(shape=self.img_shape) \n",
    "        critic_output_from_generated_samples = self.critic(generated_samples)\n",
    "        \n",
    "        real_samples = Input(shape=self.img_shape)        \n",
    "        critic_output_from_real_samples = self.critic(real_samples)\n",
    "\n",
    "        averaged_samples = Input(shape=self.img_shape)\n",
    "        critic_output_from_averaged_samples = self.critic(averaged_samples)\n",
    "\n",
    "        partial_gp_loss = partial(GradientPenaltyLoss,\n",
    "                                  averaged_samples=averaged_samples,\n",
    "                                  gradient_penalty_weight=self.λ)\n",
    "        # Functions need names or Keras will throw an error\n",
    "        partial_gp_loss.__name__ = 'gradient_penalty'\n",
    "\n",
    "        self.critic_model = Model(inputs=[generated_samples, \n",
    "                                          real_samples,\n",
    "                                          averaged_samples],\n",
    "                                  outputs=[critic_output_from_generated_samples, \n",
    "                                           critic_output_from_real_samples,\n",
    "                                           critic_output_from_averaged_samples])\n",
    "        if self.gpu_count > 1:\n",
    "            self.critic_model = multi_gpu_model(self.critic_model, gpus=self.gpu_count)\n",
    "        self.critic_model.compile(optimizer=optimizer, \n",
    "                                  loss=[WassersteinLoss, \n",
    "                                        WassersteinLoss, \n",
    "                                        partial_gp_loss])\n",
    "        \n",
    "        print('Critic Summary:')\n",
    "        self.critic.summary()       \n",
    "        \n",
    "        #-------------------------------\n",
    "        # Compile Generator\n",
    "        #-------------------------------\n",
    "        # For the generator we freeze the critic's layers\n",
    "        self.critic.trainable = False\n",
    "                    \n",
    "        generator_input = Input(shape=(self.latent_dim,))\n",
    "        generator_layers = self.generator(generator_input)\n",
    "        critic_layers_for_generator = self.critic(generator_layers)\n",
    "        \n",
    "        self.generator_model = Model(inputs=[generator_input], \n",
    "                                     outputs=[critic_layers_for_generator])\n",
    "        if self.gpu_count > 1:\n",
    "            self.generator_model = multi_gpu_model(self.generator_model, gpus=self.gpu_count)\n",
    "        self.generator_model.compile(optimizer=optimizer,\n",
    "                                     loss=WassersteinLoss)        \n",
    "\n",
    "        print('Genarator Summary:')\n",
    "        self.generator.summary()   \n",
    "\n",
    "    def build_generator(self):\n",
    "        model = Sequential()\n",
    "        model.add(Reshape((self.input_rows, self.input_cols, int(self.latent_dim / (self.input_rows * self.input_cols))), \n",
    "                          input_shape=(self.latent_dim,)\n",
    "                         ))\n",
    "\n",
    "        model.add(Conv2DTranspose(1024, (5, 5), strides=1, padding='same',\n",
    "                                 kernel_initializer=keras.initializers.Orthogonal(gain=1.4, seed=None),\n",
    "                                 ))\n",
    "        model.add(LeakyReLU(alpha=0.2))        \n",
    "        \n",
    "        model.add(Conv2DTranspose(512, (5, 5), strides=2, padding='same', \n",
    "                                 kernel_initializer=keras.initializers.Orthogonal(gain=1.4, seed=None),\n",
    "                                 ))\n",
    "        model.add(LeakyReLU(alpha=0.2))  \n",
    "        \n",
    "        model.add(Conv2DTranspose(256, (5, 5), strides=2, padding='same', \n",
    "                                 kernel_initializer=keras.initializers.Orthogonal(gain=1.4, seed=None),\n",
    "                                 ))\n",
    "        model.add(LeakyReLU(alpha=0.2))  \n",
    "\n",
    "        model.add(Conv2DTranspose(128, (5, 5), strides=2, padding='same', \n",
    "                                 kernel_initializer=keras.initializers.Orthogonal(gain=1.4, seed=None),\n",
    "                                 ))\n",
    "        model.add(LeakyReLU(alpha=0.2))  \n",
    "        \n",
    "        model.add(Conv2DTranspose(3, (5, 5), strides=2, padding='same', \n",
    "                                 kernel_initializer=keras.initializers.Orthogonal(gain=1.4, seed=None),\n",
    "                                 ))                             \n",
    "        model.add(Activation(\"tanh\"))\n",
    "\n",
    "        noise = Input(shape=(self.latent_dim,))\n",
    "        img = model(noise)\n",
    "        return Model(noise, img)\n",
    "    \n",
    "    def build_critic(self):\n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(64, (5, 5), strides=2, input_shape=self.img_shape, padding=\"same\",\n",
    "                         kernel_initializer=keras.initializers.Orthogonal(gain=1.4, seed=None),\n",
    "                        ))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        \n",
    "        model.add(Conv2D(128, (5, 5), strides=2, padding=\"same\",\n",
    "                         kernel_initializer=keras.initializers.Orthogonal(gain=1.4, seed=None),\n",
    "                        ))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "        model.add(Conv2D(256, (5, 5), strides=2, padding=\"same\",\n",
    "                         kernel_initializer=keras.initializers.Orthogonal(gain=1.4, seed=None),\n",
    "                        ))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        \n",
    "        model.add(Conv2D(512, (5, 5), strides=2, padding=\"same\",\n",
    "                         kernel_initializer=keras.initializers.Orthogonal(gain=1.4, seed=None),\n",
    "                        ))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        \n",
    "        model.add(Conv2D(1, (4, 4), strides=1, padding=\"valid\",\n",
    "                         kernel_initializer=keras.initializers.Orthogonal(gain=1.4, seed=None),\n",
    "                        ))\n",
    "        model.add(Flatten())\n",
    "\n",
    "        img = Input(shape=self.img_shape)\n",
    "        validity = model(img)\n",
    "        return Model(img, validity)\n",
    "    \n",
    "    def train(self, epochs, batch_size, sample_interval=5000, resume=0):\n",
    "        # Load suspended training weights\n",
    "        if resume != 0:\n",
    "            self.critic = load_model('./saved_model/wgangp_dc'+str(self.img_rows)+'_critic_model_'+str(resume)+'epoch.h5')\n",
    "            self.generator = load_model('./saved_model/wgangp_dc'+str(self.img_rows)+'_gen_model_'+str(resume)+'epoch.h5')\n",
    "        \n",
    "        # Rescale the dataset -1 to 1 \n",
    "        X_train = self.dataset / 127.5 - 1.0\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        valid = -np.ones((batch_size, 1), dtype=np.float32)\n",
    "        fake = np.ones((batch_size, 1), dtype=np.float32)\n",
    "        dummy = np.zeros((batch_size, 1), dtype=np.float32)\n",
    "\n",
    "        # ---------------------\n",
    "        #  Training\n",
    "        # ---------------------\n",
    "        for epoch in tqdm(range(resume, resume + epochs + 1)):\n",
    "            for _ in range(self.n_critic):\n",
    "                noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
    "                gen_imgs = self.generator.predict(noise, batch_size=batch_size)\n",
    "                \n",
    "                idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "                real_imgs = X_train[idx]\n",
    "                               \n",
    "                ε = np.random.uniform(size=(batch_size, 1,1,1))\n",
    "                ave_imgs = ε * real_imgs + (1-ε) * gen_imgs\n",
    "                \n",
    "                # Train Critic\n",
    "                d_loss = self.critic_model.train_on_batch([gen_imgs, real_imgs, ave_imgs], \n",
    "                                                          [fake, valid, dummy])\n",
    "\n",
    "            # Train Generator\n",
    "            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
    "            g_loss = self.generator_model.train_on_batch(noise, valid)\n",
    "\n",
    "            # ---------------------\n",
    "            #  Logging\n",
    "            # ---------------------\n",
    "            # Backup Model\n",
    "            if epoch != resume and epoch % sample_interval == 0:\n",
    "            # if epoch != resume and (epoch % 1000 == 0 or epoch % sample_interval == 0):\n",
    "                self.critic.save('./saved_model/wgangp_dc'+str(self.img_rows)+'_critic_model_'+str(epoch)+'epoch.h5')\n",
    "                self.generator.save('./saved_model/wgangp_dc'+str(self.img_rows)+'_gen_model_'+str(epoch)+'epoch.h5')\n",
    "            \n",
    "            # Log Loss & Histgram\n",
    "            logs = {\n",
    "                \"loss/Critic\": d_loss[0],\n",
    "                \"loss/Generator\": g_loss,\n",
    "                \"loss_Critic/D_gen\": d_loss[1],\n",
    "                \"loss_Critic/D_real\": -d_loss[2],\n",
    "                \"loss_Critic/gradient_penalty\": d_loss[3],\n",
    "                \"loss_Critic/total_loss\": d_loss[1] + d_loss[2] + d_loss[3],                \n",
    "            }\n",
    "\n",
    "            histograms = {}\n",
    "            for layer in self.critic.layers[1].layers:\n",
    "                for i in range(len(layer.get_weights())):\n",
    "                    if \"conv\" in layer.name or \"dense\" in layer.name:\n",
    "                        name = layer.name + \"/\" + str(i)\n",
    "                        value = layer.get_weights()[i]\n",
    "                        histograms[name] = value\n",
    "            \n",
    "            self.logger.log(logs=logs, histograms=histograms, epoch=epoch)\n",
    "            \n",
    "            # Log generated image samples\n",
    "            if epoch % sample_interval == 0:\n",
    "            # if epoch == 1000 or epoch == 2000 or epoch % sample_interval == 0:\n",
    "                fig, name = self.sample_images(epoch)\n",
    "                images = {name: fig}\n",
    "                self.logger.log(images=images, epoch=epoch)\n",
    "                print(\"%d [C loss: %f] [G loss: %f]\" % (epoch, d_loss[0], g_loss))\n",
    "\n",
    "    def sample_images(self, epoch):\n",
    "        r, c = 2, 3\n",
    "        if epoch == 0:\n",
    "            idx = np.random.randint(0, self.dataset.shape[0], r * c)\n",
    "            imgs = self.dataset[idx].astype(np.uint8)\n",
    "            name = \"original.png\"\n",
    "        else:\n",
    "            noise = np.random.normal(0, 1, (r * c, self.latent_dim))\n",
    "            imgs = self.generator.predict(noise, batch_size=r * c)\n",
    "            imgs = ((0.5 * imgs + 0.5) * 255).astype(np.uint8) # Rescale images 0 - 255\n",
    "            name = str(epoch) + \".png\"        \n",
    "        fig, axs = plt.subplots(r, c)\n",
    "        cnt = 0\n",
    "        for i in range(r):\n",
    "            for j in range(c):\n",
    "                if self.channels == 1:\n",
    "                    axs[i, j].imshow(imgs[cnt, :, :, 0], cmap=\"gray\")\n",
    "                else:\n",
    "                    axs[i, j].imshow(imgs[cnt, :, :, :self.channels], cmap=\"gray\")\n",
    "                axs[i, j].axis(\"off\")\n",
    "                cnt += 1\n",
    "        return fig, name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Critic Summary:\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_7 (InputLayer)         (None, 64, 64, 3)         0         \n",
      "_________________________________________________________________\n",
      "sequential_3 (Sequential)    (None, 1)                 4314753   \n",
      "=================================================================\n",
      "Total params: 4,314,753\n",
      "Trainable params: 4,314,753\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Genarator Summary:\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_8 (InputLayer)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "sequential_4 (Sequential)    (None, 64, 64, 3)         17419523  \n",
      "=================================================================\n",
      "Total params: 17,419,523\n",
      "Trainable params: 17,419,523\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "wgan = WGANGP(dataset, gpu_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/k_yonhon/anaconda3/envs/tf-gpu/lib/python3.6/site-packages/keras/engine/saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n",
      "  0%|          | 0/30001 [00:00<?, ?it/s]/home/k_yonhon/anaconda3/envs/tf-gpu/lib/python3.6/site-packages/keras/engine/training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n",
      "  0%|          | 1/30001 [00:27<225:56:14, 27.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 [C loss: -0.311274] [G loss: -0.186310]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 3821/30001 [1:01:19<6:53:50,  1.05it/s]"
     ]
    }
   ],
   "source": [
    "wgan.train(epochs=30000, batch_size=64, sample_interval=10000, resume=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "wgan.generator.save('./saved_model/wgangpd_dc64_gen_model.h5')\n",
    "wgan.critic.save('./saved_model/wgangp_dc64_critic_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
