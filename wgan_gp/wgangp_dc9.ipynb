{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "100%|██████████| 10001/10001 [15:43<00:00, 10.60it/s]\n",
    "1024 100%|██████████| 10001/10001 [5:34:12<00:00,  2.04s/it]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Input, Reshape, Flatten, Activation\n",
    "from keras.layers.convolutional import Conv2D, Conv2DTranspose\n",
    "from keras.layers.advanced_activations import ReLU, LeakyReLU\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras.utils import multi_gpu_model\n",
    "\n",
    "import keras\n",
    "import keras.backend as K\n",
    "import keras.backend.tensorflow_backend as KTF\n",
    "import tensorflow as tf\n",
    "\n",
    "import shutil, os, sys, io, random, math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import partial\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "os.chdir('/home/k_yonhon/py/Keras-GAN/wgan_gp/')\n",
    "sys.path.append(os.pardir)\n",
    "\n",
    "from tensor_board_logger import TensorBoardLogger\n",
    "from wasserstein_loss import WassersteinLoss, GradientPenaltyLoss\n",
    "\n",
    "config = tf.ConfigProto(gpu_options=tf.GPUOptions(allow_growth=True))\n",
    "session = tf.Session(config=config)\n",
    "KTF.set_session(session)\n",
    "\n",
    "# ---------------------\n",
    "#  Parameter\n",
    "# ---------------------\n",
    "gpu_count = 1\n",
    "dataset = np.load('../datasets/lfw32.npz')['arr_0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WGANGP():\n",
    "    def __init__(self, dataset, gpu_count=1):\n",
    "        # ---------------------\n",
    "        #  Parameter\n",
    "        # ---------------------\n",
    "        self.dataset = dataset\n",
    "        self.gpu_count = gpu_count\n",
    "                \n",
    "        self.img_rows = dataset.shape[1]\n",
    "        self.img_cols = dataset.shape[2]\n",
    "        self.channels = dataset.shape[3]\n",
    "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
    "\n",
    "        self.input_rows = 2\n",
    "        self.input_cols = 2\n",
    "        self.latent_dim = 128  # Noise dim\n",
    "        \n",
    "        self.n_critic = 5\n",
    "        self.λ = 10\n",
    "        optimizer = Adam(lr=0.0001, beta_1=0., beta_2=0.9, epsilon=None, decay=0.0, amsgrad=False)\n",
    "\n",
    "        # ---------------------\n",
    "        #  Load models\n",
    "        # ---------------------\n",
    "        self.critic = self.build_critic()\n",
    "        self.generator = self.build_generator()\n",
    "        \n",
    "        #  Load pretrained weights\n",
    "        '''\n",
    "        pre_gen = load_model('./saved_model/wgangp64_gen_model_3k.h5')\n",
    "        for i, layer in enumerate(self.generator.layers[1].layers):\n",
    "            if i in [i for i in range(1, int(math.log(self.img_rows / self.input_rows, 2)) * 2, 2)]:\n",
    "                layer.set_weights(pre_gen.layers[1].layers[i].get_weights())\n",
    "                layer.trainable = False\n",
    "                \n",
    "        pre_critic = load_model('./saved_model/wgangp64_critic_model_3k.h5')\n",
    "        for i, layer in enumerate(self.critic.layers[1].layers):\n",
    "            j = i - len(self.critic.layers[1].layers)\n",
    "            if j in [-i for i in range(int(math.log(self.img_rows / self.input_rows, 2)) * 2, 0, -2)]:\n",
    "                layer.set_weights(pre_critic.layers[1].layers[j].get_weights())\n",
    "                layer.trainable = False\n",
    "        '''\n",
    "        #-------------------------------\n",
    "        # Compile Critic\n",
    "        #-------------------------------    \n",
    "        generated_samples = Input(shape=self.img_shape) \n",
    "        critic_output_from_generated_samples = self.critic(generated_samples)\n",
    "        \n",
    "        real_samples = Input(shape=self.img_shape)        \n",
    "        critic_output_from_real_samples = self.critic(real_samples)\n",
    "\n",
    "        averaged_samples = Input(shape=self.img_shape)\n",
    "        critic_output_from_averaged_samples = self.critic(averaged_samples)\n",
    "\n",
    "        partial_gp_loss = partial(GradientPenaltyLoss,\n",
    "                                  averaged_samples=averaged_samples,\n",
    "                                  gradient_penalty_weight=self.λ)\n",
    "        # Functions need names or Keras will throw an error\n",
    "        partial_gp_loss.__name__ = 'gradient_penalty'\n",
    "\n",
    "        self.critic_model = Model(inputs=[generated_samples, \n",
    "                                          real_samples,\n",
    "                                          averaged_samples],\n",
    "                                  outputs=[critic_output_from_generated_samples, \n",
    "                                           critic_output_from_real_samples,\n",
    "                                           critic_output_from_averaged_samples])\n",
    "        if self.gpu_count > 1:\n",
    "            self.critic_model = multi_gpu_model(self.critic_model, gpus=self.gpu_count)\n",
    "        self.critic_model.compile(optimizer=optimizer, \n",
    "                                  loss=[WassersteinLoss, \n",
    "                                        WassersteinLoss, \n",
    "                                        partial_gp_loss])\n",
    "        \n",
    "        # print('Critic Summary:')\n",
    "        # self.critic.summary()       \n",
    "        \n",
    "        #-------------------------------\n",
    "        # Compile Generator\n",
    "        #-------------------------------\n",
    "        # For the generator we freeze the critic's layers\n",
    "        self.critic.trainable = False\n",
    "                    \n",
    "        generator_input = Input(shape=(self.latent_dim,))\n",
    "        generator_layers = self.generator(generator_input)\n",
    "        critic_layers_for_generator = self.critic(generator_layers)\n",
    "        \n",
    "        self.generator_model = Model(inputs=[generator_input], \n",
    "                                     outputs=[critic_layers_for_generator])\n",
    "        if self.gpu_count > 1:\n",
    "            self.generator_model = multi_gpu_model(self.generator_model, gpus=self.gpu_count)\n",
    "        self.generator_model.compile(optimizer=optimizer,\n",
    "                                     loss=WassersteinLoss)        \n",
    "\n",
    "        # print('Genarator Summary:')\n",
    "        # self.generator.summary()   \n",
    "\n",
    "    def build_generator(self):\n",
    "        initializer = TruncatedNormal(mean=0, stddev=0.2, seed=42)\n",
    "        \n",
    "        model = Sequential()\n",
    "        model.add(Dense(64*8*self.img_rows//8*self.input_cols//8, activation=\"relu\", input_shape=(self.latent_dim,)))\n",
    "        model.add(Reshape((self.img_rows//8, input_cols//8)))\n",
    "        model.add(Conv2DTranspose(64*4, kernel_size=5, strides=2, padding='same', kernel_initializer=initializer)\n",
    "        model.add(BatchNormalization(momentum=0.9, epsilon=1e-5))\n",
    "        for i in range(num_res_blocks):\n",
    "                  model.add(residual_block(x, base_name=base_name, block_num=i, initializer=initializer, num_channels=64*4))      \n",
    "                  \n",
    "            x = residual_block(x, base_name=base_name, block_num=i, initializer=initializer, num_channels=64*4)\n",
    "        \n",
    "        model.add()\n",
    "        model.add()\n",
    "        model.add()\n",
    "        model.add()\n",
    "        model.add()\n",
    "        \n",
    "        model.add(Reshape((self.input_rows, self.input_cols, int(self.latent_dim / (self.input_rows * self.input_cols))), \n",
    "                          input_shape=(self.latent_dim,)\n",
    "                         ))  \n",
    "        \n",
    "        for i in range(int(math.log(self.img_rows / self.input_rows, 2)) - 1):\n",
    "            model.add(Conv2DTranspose(2 ** (int(math.log(self.img_rows / self.input_rows, 2)) + 5 - i), (3+i, 3+i), strides=2, padding='same', \n",
    "                                     kernel_initializer=keras.initializers.Orthogonal(gain=1.4, seed=None),\n",
    "                                     ))\n",
    "            model.add(LeakyReLU(alpha=0.2))  \n",
    "            model.add(Conv2DTranspose(2 ** (int(math.log(self.img_rows / self.input_rows, 2)) + 5 - i), (3+i, 3+i), strides=1, padding=\"same\",\n",
    "                             kernel_initializer=keras.initializers.Orthogonal(gain=1.4, seed=None),\n",
    "                            ))\n",
    "            model.add(LeakyReLU(alpha=0.2))\n",
    "        \n",
    "        model.add(Conv2DTranspose(3, (5, 5), strides=2, padding='same', \n",
    "                                 kernel_initializer=keras.initializers.Orthogonal(gain=1.4, seed=None),\n",
    "                                 ))                             \n",
    "        model.add(Activation(\"tanh\"))\n",
    "        print('Generator Summary:')\n",
    "        model.summary()\n",
    "\n",
    "        noise = Input(shape=(self.latent_dim,))\n",
    "        img = model(noise)\n",
    "        return Model(noise, img)\n",
    "\n",
    "    h, w, c = image_shape\n",
    "\n",
    "    x = Dense(64*8*h//8*w//8, activation=\"relu\", name=base_name+\"_dense\")(in_x)\n",
    "    x = Reshape((h//8, w//8, -1))(x)\n",
    "\n",
    "    x = Conv2DTranspose(64*4, kernel_size=5, strides=2, padding='same', kernel_initializer=initializer,\n",
    "                        name=base_name + \"_deconv1\")(x)\n",
    "    x = BatchNormalization(momentum=0.9, epsilon=1e-5, name=base_name + \"_bn1\")(x, training=1)\n",
    "\n",
    "    for i in range(num_res_blocks):\n",
    "        x = residual_block(x, base_name=base_name, block_num=i, initializer=initializer, num_channels=64*4)\n",
    "\n",
    "    # size//8→size//4→size//2→size\n",
    "    x = Conv2DTranspose(64*2, kernel_size=5, strides=2, padding='same', kernel_initializer=initializer,\n",
    "                        name=base_name + \"_deconv2\")(x)\n",
    "    x = BatchNormalization(momentum=0.9, epsilon=1e-5, name=base_name + \"_bn2\")(x, training=1)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    x = Conv2DTranspose(64*1, kernel_size=5, strides=2, padding='same', kernel_initializer=initializer,\n",
    "                        name=base_name + \"_deconv3\")(x)\n",
    "    x = BatchNormalization(momentum=0.9, epsilon=1e-5, name=base_name + \"_bn3\")(x,training=1)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    out = Conv2DTranspose(3, kernel_size=7, strides=1, padding='same', activation=\"tanh\",\n",
    "                          kernel_initializer=initializer, name=base_name + \"_out\")(x)\n",
    "    \n",
    "    def build_critic(self):\n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(2 ** 7, (5, 5), strides=2, input_shape=self.img_shape, padding=\"same\",\n",
    "                         kernel_initializer=keras.initializers.Orthogonal(gain=1.4, seed=None),\n",
    "                        ))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Conv2D(2 ** 7, (5, 5), strides=1, padding=\"same\",\n",
    "                         kernel_initializer=keras.initializers.Orthogonal(gain=1.4, seed=None),\n",
    "                        ))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        \n",
    "        for i in range(int(math.log(self.img_rows / self.input_rows, 2)) - 2):\n",
    "            model.add(Conv2D(2 ** (i + 8), (4-i, 4-i), strides=2, padding=\"same\",\n",
    "                             kernel_initializer=keras.initializers.Orthogonal(gain=1.4, seed=None),\n",
    "                            ))\n",
    "            model.add(LeakyReLU(alpha=0.2))\n",
    "            model.add(Conv2D(2 ** (i + 8), (4-i, 4-i), strides=1, padding=\"same\",\n",
    "                             kernel_initializer=keras.initializers.Orthogonal(gain=1.4, seed=None),\n",
    "                            ))\n",
    "            model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "                \n",
    "        model.add(Conv2D(1, (2, 2), strides=3, padding=\"valid\",\n",
    "                         kernel_initializer=keras.initializers.Orthogonal(gain=1.4, seed=None),\n",
    "                        ))\n",
    "        model.add(Flatten())\n",
    "        print('Critic Summary:')\n",
    "        model.summary()\n",
    "\n",
    "        img = Input(shape=self.img_shape)\n",
    "        validity = model(img)\n",
    "        return Model(img, validity)\n",
    "\n",
    "    def train(self, epochs, batch_size, sample_interval=5000, resume=0):       \n",
    "        # ---------------------\n",
    "        #  for Logging\n",
    "        # ---------------------\n",
    "        target_dir = \"./lambda_search/my_log_dir8_\"+str(self.λ)\n",
    "        seed = 0\n",
    "        image_num = 5       \n",
    "        np_samples = []\n",
    "        \n",
    "        # Load suspended training weights\n",
    "        if resume != 0:\n",
    "            self.critic_model = load_model('./saved_model/wgangp8'+str(self.λ)+'_critic_model_'+str(resume)+'epoch.h5')\n",
    "            self.generator_model = load_model('./saved_model/wgangp8'+str(self.λ)+'_gen_model_'+str(resume)+'epoch.h5')\n",
    "            np_samples_npz = np.load('./saved_model/np_samples8'+str(self.λ)+'_'+str(resume)+'epoch.npz')\n",
    "            for i, np_sample in enumerate(np_samples_npz):\n",
    "                np_samples.append(np_sample)\n",
    "        else:            \n",
    "            shutil.rmtree(target_dir, ignore_errors=True)\n",
    "            os.mkdir(target_dir)\n",
    "                \n",
    "        self.logger = TensorBoardLogger(log_dir=target_dir)            \n",
    "        \n",
    "        # ---------------------\n",
    "        #  Training\n",
    "        # ---------------------\n",
    "        # Rescale the dataset -1 to 1 \n",
    "        X_train = self.dataset / 127.5 - 1.0\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        valid = -np.ones((batch_size, 1), dtype=np.float32)\n",
    "        fake = np.ones((batch_size, 1), dtype=np.float32)\n",
    "        dummy = np.zeros((batch_size, 1), dtype=np.float32)\n",
    "\n",
    "        for epoch in tqdm(range(resume, resume + epochs + 1)):\n",
    "            for _ in range(self.n_critic):\n",
    "                noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
    "                gen_imgs = self.generator.predict(noise, batch_size=batch_size)\n",
    "                \n",
    "                idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "                real_imgs = X_train[idx]\n",
    "                               \n",
    "                ε = np.random.uniform(size=(batch_size, 1,1,1))\n",
    "                ave_imgs = ε * real_imgs + (1-ε) * gen_imgs\n",
    "                \n",
    "                # Train Critic\n",
    "                d_loss = self.critic_model.train_on_batch([gen_imgs, real_imgs, ave_imgs], \n",
    "                                                          [fake, valid, dummy])\n",
    "\n",
    "            # Train Generator\n",
    "            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
    "            g_loss = self.generator_model.train_on_batch(noise, valid)\n",
    "\n",
    "            # ---------------------\n",
    "            #  Logging\n",
    "            # ---------------------\n",
    "            # Backup Model\n",
    "            '''\n",
    "            if epoch != resume and epoch % sample_interval == 0:\n",
    "                self.critic_model.save('./saved_model/wgangp'+str(self.λ)+'_critic_model_'+str(epoch)+'epoch.h5')\n",
    "                self.generator_model.save('./saved_model/wgangp'+str(self.λ)+'_gen_model_'+str(epoch)+'epoch.h5')\n",
    "                np.savez_compressed('./saved_model/np_samples'+str(self.λ)+'_'+str(epoch)+'epoch.npz', np_samples)\n",
    "            '''\n",
    "            # Log Loss & Histgram\n",
    "            logs = {\n",
    "                \"loss/Critic\": d_loss[0],\n",
    "                \"loss/Generator\": g_loss,\n",
    "                \"loss_Critic/D_gen\": d_loss[1],\n",
    "                \"loss_Critic/D_real\": -d_loss[2],\n",
    "                \"loss_Critic/gradient_penalty\": d_loss[3],\n",
    "                \"loss_Critic/total_loss\": d_loss[1] + d_loss[2] + d_loss[3],                \n",
    "            }\n",
    "\n",
    "            histograms = {}\n",
    "            '''\n",
    "            for layer in self.critic.layers[1].layers:\n",
    "                for i in range(len(layer.get_weights())):\n",
    "                    if \"conv\" in layer.name or \"dense\" in layer.name:\n",
    "                        name = layer.name + \"/\" + str(i)\n",
    "                        value = layer.get_weights()[i]\n",
    "                        histograms[name] = value\n",
    "            '''\n",
    "            self.logger.log(logs=logs, histograms=histograms, epoch=epoch)\n",
    "            \n",
    "            # Log generated image samples\n",
    "            if epoch % sample_interval == 0:\n",
    "                np.random.seed(seed)\n",
    "                noise = np.random.normal(0, 1, (image_num, self.latent_dim))\n",
    "                gen_imgs = self.generator.predict(noise)\n",
    "                gen_imgs = ((0.5 * gen_imgs + 0.5) * 255).astype(np.uint8)\n",
    "                np_samples.append(gen_imgs)\n",
    "                '''\n",
    "                fig, name = self.sample_images(epoch)\n",
    "                images = {epoch: fig}\n",
    "                self.logger.log(images=images, epoch=epoch)\n",
    "                '''\n",
    "                print(\"%d [C loss: %f] [G loss: %f]\" % (epoch, d_loss[0], g_loss))\n",
    "                \n",
    "        return np_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Critic Summary:\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_15 (Conv2D)           (None, 16, 16, 128)       9728      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_25 (LeakyReLU)   (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 16, 16, 128)       409728    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_26 (LeakyReLU)   (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 8, 8, 256)         524544    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_27 (LeakyReLU)   (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 8, 8, 256)         1048832   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_28 (LeakyReLU)   (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_19 (Conv2D)           (None, 4, 4, 512)         1180160   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_29 (LeakyReLU)   (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_20 (Conv2D)           (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_30 (LeakyReLU)   (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_21 (Conv2D)           (None, 1, 1, 1)           2049      \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 5,534,849\n",
      "Trainable params: 5,534,849\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Generator Summary:\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "reshape_3 (Reshape)          (None, 2, 2, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_15 (Conv2DT (None, 4, 4, 512)         147968    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_31 (LeakyReLU)   (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_16 (Conv2DT (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_32 (LeakyReLU)   (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_17 (Conv2DT (None, 8, 8, 256)         2097408   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_33 (LeakyReLU)   (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_18 (Conv2DT (None, 8, 8, 256)         1048832   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_34 (LeakyReLU)   (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_19 (Conv2DT (None, 16, 16, 128)       819328    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_35 (LeakyReLU)   (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_20 (Conv2DT (None, 16, 16, 128)       409728    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_36 (LeakyReLU)   (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_21 (Conv2DT (None, 32, 32, 3)         9603      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 32, 32, 3)         0         \n",
      "=================================================================\n",
      "Total params: 6,892,675\n",
      "Trainable params: 6,892,675\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "gan = WGANGP(dataset, gpu_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/10001 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "You must compile a model before training/testing. Use `model.compile(optimizer, loss)`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-eaa9d5634704>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresume\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-081c69ef88d6>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, epochs, batch_size, sample_interval, resume)\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;31m# Train Critic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m                 d_loss = self.critic_model.train_on_batch([gen_imgs, real_imgs, ave_imgs], \n\u001b[0;32m--> 203\u001b[0;31m                                                           [fake, valid, dummy])\n\u001b[0m\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m             \u001b[0;31m# Train Generator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf-gpu/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1209\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1210\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1211\u001b[0;31m             class_weight=class_weight)\n\u001b[0m\u001b[1;32m   1212\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_uses_dynamic_learning_phase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1213\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf-gpu/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    679\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 681\u001b[0;31m                 raise RuntimeError('You must compile a model before '\n\u001b[0m\u001b[1;32m    682\u001b[0m                                    \u001b[0;34m'training/testing. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    683\u001b[0m                                    'Use `model.compile(optimizer, loss)`.')\n",
      "\u001b[0;31mRuntimeError\u001b[0m: You must compile a model before training/testing. Use `model.compile(optimizer, loss)`."
     ]
    }
   ],
   "source": [
    "np_samples = gan.train(epochs=10000, batch_size=64, sample_interval=1000, resume=10000)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "gan.generator_model.save('./saved_model/wgangp8'+str(gan.λ)+'_gen_model_'+str(10000)+'epoch.h5')\n",
    "gan.critic_model.save('./saved_model/wgangp8'+str(gan.λ)+'_critic_model_'+str(10000)+'epoch.h5')\n",
    "np.savez_compressed('./saved_model/np_samples8'+str(gan.λ)+'_'+str(10000)+'epoch.npz', np_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import holoviews as hv\n",
    "hv.notebook_extension()\n",
    "for j in range(1, 11):\n",
    "    y = np_samples[j]\n",
    "    for i in range(5):\n",
    "        if j == 1 and i == 0:\n",
    "            hv_points = hv.RGB(y[i]).relabel(str(j*1000)+' epoch')\n",
    "        else:\n",
    "            hv_points += hv.RGB(y[i]).relabel(str(j*1000)+' epoch')\n",
    "hv_points.cols(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = np.random.normal(0, 1, (10, gan.latent_dim))\n",
    "gen_imgs = gan.generator.predict(noise)\n",
    "y = ((0.5 * gen_imgs + 0.5) * 255).astype(np.uint8)\n",
    "for j in range(5):\n",
    "    if j == 0:\n",
    "        hv_points = hv.RGB(y[j])\n",
    "    else:\n",
    "        hv_points += hv.RGB(y[j])\n",
    "hv_points.cols(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
